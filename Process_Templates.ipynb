{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting positive/negative template pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows from the `MatchTemplatesUDF` notebook, which outputs a set of `{template_name}-just-reverts-v2.csv.gz files.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mwparserfromhell as mwp\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import csv\n",
    "from math import ceil\n",
    "import mwapi  \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_reverts(infname, outfname, col_names, chunksize):\n",
    "    '''\n",
    "    Filters out revisions which were reverted\n",
    "    '''\n",
    "\n",
    "    # First filter out reverted entries\n",
    "    chunksize=10000\n",
    "    for chunk in pd.read_csv(infname, sep='\\t', lineterminator='\\n', names=col_names, header=None, chunksize=chunksize):\n",
    "        #chunk.revision_is_identity_reverted = chunk.revision_is_identity_reverted.map({'true':True, 'false':False})\n",
    "        not_reverted = chunk[chunk.revision_is_identity_reverted==False]\n",
    "\n",
    "        # Record to CSVs\n",
    "        if not os.path.isfile(outfname):\n",
    "            with open(outfname, 'w') as f:\n",
    "                not_reverted.to_csv(f)\n",
    "        else:\n",
    "            with open(outfname, 'a') as f:\n",
    "                not_reverted.to_csv(f, header=False)\n",
    "\n",
    "                \n",
    "def filter_valid_pages(infname, outfname, col_names):\n",
    "    '''\n",
    "    Filters out pages which never had legitimate instances of template addition\n",
    "    I.e template additions that were ultimately reverted\n",
    "    '''\n",
    "    page_dumps = pd.read_csv(infname, usecols=col_names)\n",
    "    print(len(page_dumps))\n",
    "    page_dumps.drop_duplicates(subset=['revision_id'], inplace=True)\n",
    "    page_dumps.dropna(subset=['revision_id'], inplace=True)\n",
    "    page_dumps = page_dumps.astype({'page_id':'int64', 'revision_id':'int64'})\n",
    "    \n",
    "    # Filter out pages that never had this template that wasn't reverted\n",
    "    has_temp_revs = page_dumps[page_dumps['has_template']==1.0]\n",
    "    has_template_pageIDs = list(set(has_temp_revs.page_id))\n",
    "    page_dumps_idx = page_dumps.set_index('page_id')\n",
    "    pdumps = page_dumps_idx[page_dumps_idx.index.isin(has_template_pageIDs)]\n",
    "    pdumps['page_id'] = pdumps.index\n",
    "    pdumps['page_id'] = pdumps.index\n",
    "    pdumps.reset_index(drop=True, inplace=True)\n",
    "    pdumps = pdumps[['page_id', 'revision_id', 'revision_minor_edit', 'revision_text_bytes', 'event_user_is_anonymous', 'has_template']]\n",
    "    \n",
    "    print(\"\\t\\tRevision counts after filtering: {} ==> {}\".format(len(page_dumps), len(pdumps)))\n",
    "    print(\"\\t\\tPage counts after filtering: {} ==> {}\".format(len(np.unique(page_dumps.page_id)), len(np.unique(pdumps.page_id))))\n",
    "\n",
    "    pdumps.to_csv(outfname, compression='gzip')\n",
    "    \n",
    "    \n",
    "def extract_template_pairs(page_df):\n",
    "    '''\n",
    "    Extract pairs of positive/negative template examples\n",
    "    '''\n",
    "    page_df = page_df.sort_values('revision_id')\n",
    "    has_template = False\n",
    "    positive_record = None\n",
    "    \n",
    "    for _, revision in page_df.iterrows():\n",
    "        if not has_template and revision.has_template:\n",
    "            #Record first positive instance\n",
    "            positive_record = revision\n",
    "            has_template = True\n",
    "        \n",
    "        if has_template and not revision.has_template:\n",
    "            #Template removed, record negative instance\n",
    "            negative_record = revision\n",
    "            \n",
    "            # Record ID of corresponding pair\n",
    "            positive_record['revision_id.key'] = negative_record.revision_id\n",
    "            negative_record['revision_id.key'] = positive_record.revision_id\n",
    "            \n",
    "            yield positive_record, negative_record\n",
    "            \n",
    "            #Reset variables            \n",
    "            has_template = False\n",
    "            positive_record = None\n",
    "    \n",
    "def extract_templatepairs_from_pages(infname, outfname, in_fieldnames, out_fieldnames):\n",
    "    pdumps = pd.read_csv(infname, usecols=in_fieldnames)\n",
    "\n",
    "    with open(outfname, \"wt\") as csvfile: \n",
    "        csvwrite = csv.DictWriter(csvfile, delimiter=',', fieldnames=out_fieldnames)\n",
    "        csvwrite.writeheader() \n",
    "     \n",
    "        # Process for pos and neg examples\n",
    "        pages = pdumps.groupby('page_id')\n",
    "\n",
    "        with tqdm(total=len(pages)) as progbar:\n",
    "            for page_id, page in pages:\n",
    "                for pos_record, neg_record in extract_template_pairs(page):\n",
    "                    csvwrite.writerow(dict(pos_record))\n",
    "                    csvwrite.writerow(dict(neg_record))\n",
    "                progbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESS TEMPLATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each template file:\n",
    "\n",
    "    1. Filter out reverted revisions\n",
    "    2. Filter out remaining pages which still have the respective templates\n",
    "    3. Extract positive/negative revision pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "RELIABILITY_TEMPLATES_DIR = 'YOUR-FOLDER-HERE' #outputs of MatchTemplatesUDF notebook\n",
    "TEMPLATES_DIR = 'OUTPUT-FOLDER' #Destination of output folder\n",
    "TMP_DIR= TEMPLATES_DIR+'/tmp'\n",
    "TEMPLATE_PAIRS_DIR = TEMPLATES_DIR+'/template_pairs'\n",
    "TEMPLATE_FEATURES_DIR = TEMPLATES_DIR+'/features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {TEMPLATES_DIR}\n",
    "!mkdir {TMP_DIR}\n",
    "!mkdir {TEMPLATE_PAIRS_DIR}\n",
    "!mkdir {TEMPLATE_FEATURES_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Wikimedia API session\n",
    "LANGUAGE = 'enwiki'\n",
    "SITENAME = LANGUAGE.replace('wiki', '.wikipedia')\n",
    "\n",
    "session = mwapi.Session('https://{0}.org'.format(SITENAME), user_agent='{0} -- {1}'.format('Outreachy Templates (mwapi)', '0xkaywong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_NAMES = ['event_timestamp', 'page_title', 'page_id', 'revision_id', 'revision_is_identity_reverted', \\\n",
    "             'revision_minor_edit', 'revision_text_bytes', 'revision_first_identity_reverting_revision_id',\\\n",
    "             'revision_seconds_to_identity_revert', 'event_user_is_anonymous', 'has_template', 'event_comment']\n",
    "\n",
    "# Get list of all template names\n",
    "TEMPLATE_NAMES = glob.glob(RELIABILITY_TEMPLATES_DIR+'/*.csv.gz')\n",
    "TEMPLATE_NAMES = [p.split(TEMPLATES_DIR+'/raw/')[1].split('-just-reverts')[0] for p in TEMPLATE_NAMES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for template_name in TEMPLATE_NAMES:\n",
    "    print(\"\\n========================================================================================================\")\n",
    "    print(\"PROCESSING {}\\n\".format(template_name))\n",
    "    print(\"========================================================================================================\")\n",
    "    \n",
    "    # Filter reverted revisions\n",
    "    template_fname = RELIABILITY_TEMPLATES_DIR+\"/\"+ template_name +'-just-reverts-v2.csv.gz'\n",
    "    unreverted_outfname =  TMP_DIR+'/'+template_name+'_unreverted.csv'\n",
    "    print(\"\\n*************************************************\")\n",
    "    print(\"FILTERING REVERTS \\n\\tFOR FILE {}... \".format(template_fname))\n",
    "    start = time.time()\n",
    "    filter_reverts(template_fname, unreverted_outfname, COL_NAMES, chunksize=10000)\n",
    "    end = time.time()\n",
    "    print(\"\\tWROTE TO {} \\n\\tRUNTIME: {}\".format(unreverted_outfname, end-start))\n",
    "    \n",
    "    # Filter pages\n",
    "    col_subset = ['page_id', 'revision_id', 'revision_minor_edit', 'revision_text_bytes', 'event_user_is_anonymous', 'has_template']\n",
    "    filtered_outfname = TMP_DIR+'/'+template_name+'_unreverted_filtered.csv.gz'\n",
    "    print(\"\\n******************************************************************\")\n",
    "    print(\"FILTERING PAGES WITHOUT TEMPLATES \\n\\tFOR FILE {}... \".format(unreverted_outfname))\n",
    "    start = time.time()\n",
    "    filter_valid_pages(unreverted_outfname, filtered_outfname, col_subset)\n",
    "    end = time.time()\n",
    "    print(\"\\tWROTE TO {} \\n\\tRUNTIME: {}\".format(filtered_outfname, end-start))\n",
    "\n",
    "    # Extract posneg examples\n",
    "    col_subset_out = ['page_id', 'revision_id', 'revision_id.key', 'revision_minor_edit', 'revision_text_bytes', 'event_user_is_anonymous', 'has_template']\n",
    "    outfname = TEMPLATE_PAIRS_DIR+'/'+template_name+'_pairs.csv'\n",
    "    print(\"\\n*************************************************\")\n",
    "    print(\"EXTRACTING POS & NEG TEMPLATE PAIRS \\n\\tFOR FILE {}... \".format(filtered_outfname))\n",
    "    start = time.time()\n",
    "    extract_templatepairs_from_pages(filtered_outfname, outfname, col_subset, col_subset_out)\n",
    "    end = time.time()\n",
    "    print(\"\\tWROTE TO {} \\n\\tRUNTIME: {}\".format(outfname, end-start))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {TEMPLATE_NAMES}/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query API for more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after obtaining the datasets of positive/negative example pairs, we query the ORES API for additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ores_features(modelname, revids_list):\n",
    "    '''\n",
    "    Query ORES for features of models in `model_list`,\n",
    "    for all revisions in `revids_list`\n",
    "    '''\n",
    "    revids_str = \"|\".join(map(str,revids_list))\n",
    "    ores_q =  \"http://ores.wikimedia.org/v3/scores/enwiki/?models={}&features&revids={}\".format(modelname, revids_str)\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(ores_q)\n",
    "        res = r.json()\n",
    "        res = pd.DataFrame(res['enwiki']['scores']).T\n",
    "    except Exception as e:\n",
    "        print(\"Query ORES features error: {}\".format(e))\n",
    "        return None\n",
    "\n",
    "    return res\n",
    "\n",
    "def query_and_record(modelname, revids_list):\n",
    "    '''\n",
    "    Queries ORES for features of models in `model_list`, for all revisions in `revids_list`\n",
    "    and record to CSVs\n",
    "    '''\n",
    "    res = query_ores_features(modelname, revids_list)\n",
    "    try:\n",
    "        res_model = res[modelname] \n",
    "        df_model= pd.json_normalize(list(res_model))\n",
    "        df_model['revision_id'] = res_model.index\n",
    "        df_model.set_index('revision_id', inplace=True)\n",
    "\n",
    "        try: #if the following columns exist, drop\n",
    "            df_model.drop(['error.message', 'error.type'], axis=1, inplace=True)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        df_model['revision_id']=df_model.index\n",
    "        res_dict = df_model.to_dict(orient='records')\n",
    "\n",
    "        return res_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Query and record error| {}: {}\".format(modelname, e))\n",
    "        return None\n",
    "        \n",
    "\n",
    "def query_and_record_chunked(modelname, rev_ids):\n",
    "    \"\"\"\n",
    "    Queries ORES API for list of pageIDs\n",
    "    models_list: List of models to obtain features for\n",
    "    rev_ids: List of revision IDs to chunk\n",
    "    #The max number of IDs that can be queried at once is 50, \n",
    "    #so we have to chunk our list of revIDs into lists of 50 and loop\n",
    "    \"\"\"\n",
    "    res_full = []\n",
    "    revid_chunks = [rev_ids[i:i + 50] for i in range(0, len(rev_ids),50)] \n",
    "    \n",
    "    with tqdm(total=len(revid_chunks)) as progbar:\n",
    "        for revid_chunk in revid_chunks:\n",
    "            chunk_res = query_and_record(modelname, revid_chunk)\n",
    "            if chunk_res is not None:\n",
    "                res_full.extend(chunk_res)\n",
    "            progbar.update(1)\n",
    "            \n",
    "    return res_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_FNAMES = glob.glob(TEMPLATE_PAIRS_DIR+'/*.csv')\n",
    "\n",
    "for template_fname in TEMPLATE_FNAMES:\n",
    "    print(\"\\n========================================================================================================\")\n",
    "    print(\"PROCESSING {}\\n\".format(template_fname))\n",
    "    print(\"========================================================================================================\")\n",
    "    template_name= template_fname.split(TEMPLATE_PAIRS_DIR+'/')[1].split('_pairs')[0]\n",
    "    features_outfname = TEMPLATE_FEATURES_DIR+'{}_features.csv'.format(template_name)\n",
    "\n",
    "    # Read in data\n",
    "    df  = pd.read_csv(template_fname)\n",
    "    df['template'] = template_name\n",
    "\n",
    "    #Drop instances of page blanking vandalism\n",
    "    blanked_idx = list(df[df.revision_text_bytes==0].index)\n",
    "    print(\"Blanking vandalism instances found: {}\".format(len(blanked_idx)))\n",
    "    blanked_idx.extend([idx-1 for idx in blanked_idx])\n",
    "    blanked_idx = np.sort(blanked_idx)\n",
    "    df.drop(blanked_idx, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Query article_quality API for more features\n",
    "    revids = list(df.revision_id)\n",
    "    modelname = 'articlequality'\n",
    "    res_full = query_and_record_chunked(modelname, revids)\n",
    "    res_full = pd.DataFrame(res_full)\n",
    "\n",
    "    #Merge with original df\n",
    "    df['revision_id'] = df.revision_id.astype(str)\n",
    "    df_full = df.merge(res_full, left_on='revision_id', right_on='revision_id')\n",
    "\n",
    "    # Rename feature columns\n",
    "    rename_map={}\n",
    "    for featurename in df_full.columns[df_full.columns.str.startswith('features')]:\n",
    "        rename_map[featurename] = featurename.split('features.feature.')[1]\n",
    "\n",
    "    for featurename in df_full.columns[df_full.columns.str.startswith('score')]:\n",
    "        rename_map[featurename] = 'article_quality.'+featurename\n",
    "    df_full.rename(columns=rename_map, inplace=True)\n",
    "    \n",
    "    # Drop score probability columns\n",
    "    drop_colnames= list(df_full.columns[df_full.columns.str.startswith('article_quality.score.probability')])\n",
    "    df_full.drop(drop_colnames, axis=1, inplace=True)\n",
    "\n",
    "    # Rename remaining feature columns\n",
    "    col_map={'revision_text_bytes': 'revision_text_bytes',\n",
    "             'english.stemmed.revision.stems_length': 'stems_length',\n",
    "             'enwiki.revision.category_links': 'category_links',\n",
    "             'enwiki.revision.cite_templates': 'cite_templates',\n",
    "             'enwiki.revision.cn_templates': 'cn_templates',\n",
    "             'enwiki.revision.images_in_tags': 'images_in_tags',\n",
    "             'enwiki.revision.infobox_templates': 'infobox_templates',\n",
    "             'enwiki.revision.paragraphs_without_refs_total_length': 'paragraphs_without_refs',\n",
    "             'enwiki.revision.shortened_footnote_templates': 'shortened_footnote_templates',\n",
    "             'enwiki.revision.who_templates': 'who_templates',\n",
    "             'len(<datasource.english.words_to_watch.revision.matches>)': 'words_to_watch_matches',\n",
    "             'len(<datasource.wikitext.revision.words>)': 'revision_words',\n",
    "             'wikitext.revision.chars': 'revision_chars',\n",
    "             'wikitext.revision.content_chars': 'revision_content_chars',\n",
    "             'wikitext.revision.external_links': 'external_links',\n",
    "             'wikitext.revision.headings_by_level(2)': 'headings_by_level(2)',\n",
    "             'wikitext.revision.ref_tags': 'ref_tags',\n",
    "             'wikitext.revision.templates': 'revision_templates',\n",
    "             'wikitext.revision.wikilinks': 'revision_wikilinks',\n",
    "             'article_quality.score.prediction': 'article_quality_score'}\n",
    "\n",
    "    df_full.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # Record to CSVs\n",
    "    with open(features_outfname, 'w') as f:\n",
    "        df_full.to_csv(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
