{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting reliability templates from mediawiki_wikitext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get reliability related templates\n",
    "templates = [l.strip() for l in  open('templates_selfPromotion.txt')]\n",
    "#config folders\n",
    "outputHDFS = 'YOUR-FOLDER-HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `collaborationPatterns': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir $outputHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re \n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def getTemplatesRegex(wikitext):\n",
    "    \"\"\"Extract list of templates from wikitext for an article via simple regex.\n",
    "    Known Issues:\n",
    "    * Doesn't handle nested templates (just gets the first)\n",
    "    -- e.g., '{{cite web|url=http://www.allmusic.com/|ref={{harvid|AllMusic}}}}' would be just web\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return list(set([m.split('|')[0].strip() for m in re.findall('(?<=\\{\\{)(.*?)(?=\\}\\})', wikitext, flags=re.DOTALL)]))\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "@udf(returnType=ArrayType(StringType()))\n",
    "def getTemplatesRegexRelaibility(wikitext):\n",
    "    \"\"\"\n",
    "    Same function than getTemplatesRegex, but filtered by list of templates\n",
    "    TODO: Check how to call another function (getTemplatesRegex) from here.\n",
    "    \"\"\"\n",
    "    global templates\n",
    "    try:\n",
    "        all_templates = list(set([m.split('|')[0].strip() for m in re.findall('(?<=\\{\\{)(.*?)(?=\\}\\})', wikitext, flags=re.DOTALL)]))\n",
    "        matching_templates = [template for template in all_templates if template.lower() in templates]\n",
    "        if len(matching_templates) > 0:\n",
    "            return matching_templates\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot =\"2020-09\"\n",
    "wikidb = \"enwiki\"\n",
    "wikitext_history = spark.sql('''SELECT page_id,revision_id,revision_text,page_namespace FROM wmf.mediawiki_wikitext_history \n",
    "    WHERE snapshot =\"{snapshot}\" and wiki_db =\"{wikidb}\"'''.format(wikidb=wikidb,snapshot=snapshot))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply getTemplatesRegexRelaibility over all wikitext history\n",
    "wikitext_history = wikitext_history.withColumn(\"templates\",getTemplatesRegexRelaibility(col('revision_text')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "revisions_with_template = wikitext_history.select(wikitext_history.page_id,wikitext_history.revision_id,explode(wikitext_history.templates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions_with_template.write.parquet(outputHDFS+'/templates.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Dumps\n",
    "\n",
    "* Given all the pages containing a template, generate the full list of revisions for that list\n",
    "* Enrich that list with additional meta information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions_with_template = spark.read.parquet(outputHDFS+'/templates.parquet')\n",
    "#revisions_with_template.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|          col| count|\n",
      "+-------------+------+\n",
      "|       advert|968433|\n",
      "|       Advert|755355|\n",
      "|      peacock|310418|\n",
      "|       weasel|302263|\n",
      "|       Weasel|221102|\n",
      "|      Peacock|185196|\n",
      "|       fanpov|120206|\n",
      "|autobiography| 92810|\n",
      "|       Fanpov| 74542|\n",
      "|Autobiography| 25031|\n",
      "|       FanPOV|   212|\n",
      "|       WEASEL|    98|\n",
      "|       ADVERT|    25|\n",
      "|      PEACOCK|    18|\n",
      "|       fanPOV|    11|\n",
      "|AUTOBIOGRAPHY|     2|\n",
      "|       AdVert|     1|\n",
      "|autoBiography|     1|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revisions_with_template.select('col').groupBy('col').count().orderBy('count', ascending=False).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|col          |count |\n",
      "+-------------+------+\n",
      "|advert       |968433|\n",
      "|Advert       |755355|\n",
      "|peacock      |310418|\n",
      "|weasel       |302263|\n",
      "|Weasel       |221102|\n",
      "|Peacock      |185196|\n",
      "|fanpov       |120206|\n",
      "|autobiography|92810 |\n",
      "|Fanpov       |74542 |\n",
      "|Autobiography|25031 |\n",
      "|FanPOV       |212   |\n",
      "|WEASEL       |98    |\n",
      "|ADVERT       |25    |\n",
      "|PEACOCK      |18    |\n",
      "|fanPOV       |11    |\n",
      "|AUTOBIOGRAPHY|2     |\n",
      "|AdVert       |1     |\n",
      "|autoBiography|1     |\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "revisions_with_template.select('col').groupBy('col').count().orderBy('count', ascending=False).show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobiography\n",
      "read table, done 0.0330965518951416\n",
      "save table, done 23.176331043243408\n",
      "21/02/13 18:33:18 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 20.40220880508423\n",
      "advert\n",
      "read table, done 0.04973602294921875\n",
      "save table, done 80.69189429283142\n",
      "21/02/13 18:35:00 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 116.54059410095215\n",
      "fanpov\n",
      "read table, done 0.05591607093811035\n",
      "save table, done 23.369151830673218\n",
      "21/02/13 18:37:20 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 22.441009998321533\n",
      "peacock\n",
      "read table, done 0.050488948822021484\n",
      "save table, done 70.40525674819946\n",
      "21/02/13 18:38:53 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 57.17970681190491\n",
      "weasel\n",
      "read table, done 0.05831432342529297\n",
      "save table, done 82.42752432823181\n",
      "21/02/13 18:41:12 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 90.90377163887024\n"
     ]
    }
   ],
   "source": [
    "from time  import time\n",
    "\n",
    "\n",
    "pages_templates_subset = revisions_with_template.select('page_id').distinct()\n",
    "pages_templates_subset.createOrReplaceTempView('pages_templates_subset')\n",
    "\n",
    "mediawiki_history_subset =  spark.sql('''\n",
    "        SELECT w.event_timestamp, w.page_title,w.page_id,w.page_namespace, \n",
    "        w.revision_id, w.revision_is_identity_reverted, \n",
    "        w.revision_minor_edit, w.revision_text_bytes, \n",
    "        w.revision_first_identity_reverting_revision_id, w.revision_seconds_to_identity_revert,\n",
    "        w.event_user_id,w.event_user_registration_timestamp, \n",
    "        w.event_user_is_anonymous,w.event_user_revision_count,\n",
    "\n",
    "        w.event_comment\n",
    "        FROM wmf.mediawiki_history w\n",
    "        WHERE w.snapshot =\"2020-09\" and w.wiki_db =\"enwiki\" AND  \n",
    "      w.event_entity = 'revision' AND w.page_id IN (\n",
    "                    SELECT  page_id FROM pages_templates_subset)                   \n",
    "        ''')\n",
    "mediawiki_history_subset.cache()\n",
    "mediawiki_history_subset.createOrReplaceTempView('mediawiki_history_subset')\n",
    "\n",
    "for template in templates:\n",
    "    try:\n",
    "        t1 = time()\n",
    "        print(template)\n",
    "        df = revisions_with_template.where(revisions_with_template['col']==template)\n",
    "        df.cache()\n",
    "        t2 = time()\n",
    "        print('read table, done',t2-t1)\n",
    "        t1 = time()        \n",
    "        page_ids = df.select('page_id').distinct()\n",
    "        page_ids.createOrReplaceTempView('tmp_page_ids')\n",
    "        revision_ids = df.select('revision_id').distinct()\n",
    "        revision_ids.createOrReplaceTempView('tmp_revision_ids')\n",
    "        reverts= spark.sql('''\n",
    "        SELECT w.event_timestamp, w.page_title,w.page_id, w.page_namespace,\n",
    "        w.revision_id, w.revision_is_identity_reverted, \n",
    "        w.revision_minor_edit, w.revision_text_bytes, \n",
    "        w.revision_first_identity_reverting_revision_id, w.revision_seconds_to_identity_revert,\n",
    "        w.event_user_id,w.event_user_registration_timestamp, \n",
    "        w.event_user_is_anonymous,w.event_user_revision_count,\n",
    "       CASE WHEN r.revision_id IS NOT NULL  THEN 1 ELSE 0 END has_template,\n",
    "        w.event_comment\n",
    "       \n",
    "FROM mediawiki_history_subset w LEFT OUTER JOIN tmp_revision_ids r \n",
    "                            ON (w.revision_id = r.revision_id)\n",
    "\n",
    "WHERE  w.page_id IN (\n",
    "                    SELECT  page_id FROM tmp_page_ids) \n",
    "ORDER BY page_id, w.revision_id\n",
    "''') \n",
    "        reverts.repartition(1).write.csv(outputHDFS+'/'+template,header=True,mode='overwrite',sep='\\t')\n",
    "        t2 = time()\n",
    "        print('save table, done',t2-t1)\n",
    "        t1 = time()   \n",
    "        templateout = template.replace(' ','_')\n",
    "        !hadoop fs -text \"$outputHDFS/$template/*\"  | gzip > $outputHDFS-$template-meta-info.csv.gz\n",
    "        t2 = time()\n",
    "        print('-----',t2-t1)\n",
    "    except Exception as e:\n",
    "        print('error',e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 307394: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 345644: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 923094: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 1085902: expected 16 fields, saw 17\\nSkipping line 1085903: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 1169025: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 1620566: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 1746451: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 1776388: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 2037272: expected 16 fields, saw 18\\n'\n",
      "b'Skipping line 3116407: expected 16 fields, saw 17\\n'\n",
      "b'Skipping line 3187195: expected 16 fields, saw 18\\n'\n",
      "b'Skipping line 3413792: expected 16 fields, saw 17\\n'\n",
      "/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (5,6,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('collaborationPatterns-autobiography-meta-info.csv.gz',compression='gzip',sep='\\t',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "?reverts.write.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
