{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9309adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwapi\n",
    "import wmfdata\n",
    "import requests\n",
    "import re\n",
    "import glob\n",
    "from time import time\n",
    "import mwparserfromhell as mwp\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1e6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG\n",
    "LANGUAGE_CODE='id'\n",
    "TEMPLATES_FILE_EN='../data/reliability_templates_list.txt'\n",
    "SNAPSHOT =\"2021-04\"\n",
    "\n",
    "#HADOOP OUTPUT DIRS\n",
    "outputHDFS = './HDFSout/'\n",
    "REVISIONS_OUTDIR=outputHDFS+'/revisions_template/{}/'.format(LANGUAGE_CODE)\n",
    "META_OUTDIR=outputHDFS+'/revisions_meta/{}/'.format(LANGUAGE_CODE)\n",
    "PAIRS_OUTDIR=outputHDFS+'/revisions_pairs/{}/'.format(LANGUAGE_CODE)\n",
    "PAIRS_OUTDIR_lang = PAIRS_OUTDIR+'/{}/'.format(LANGUAGE_CODE)\n",
    "PAIRS_OUTDIR_en = PAIRS_OUTDIR+'/{}/'.format('en')\n",
    "TEXT_OUTDIR=outputHDFS+'/revisions_txt/{}/'.format(LANGUAGE_CODE)\n",
    "FEATURES_OUTDIR=outputHDFS+'/revisions_features/{}/'.format(LANGUAGE_CODE)\n",
    "\n",
    "#LOCAL OUTPUT DIRS\n",
    "OUTDIR_LOCAL = '../data/out/'\n",
    "META_OUTDIR_LOCAL = OUTDIR_LOCAL+'/revisions_meta/{}/'.format(LANGUAGE_CODE)\n",
    "\n",
    "PAIRS_OUTDIR_LOCAL=OUTDIR_LOCAL+'/revisions_pairs/{}/'.format(LANGUAGE_CODE)\n",
    "PAIRS_OUTDIR_lang_LOCAL = PAIRS_OUTDIR_LOCAL+'/{}/'.format(LANGUAGE_CODE)\n",
    "PAIRS_OUTDIR_en_LOCAL = PAIRS_OUTDIR_LOCAL+'/{}/'.format('en')\n",
    "\n",
    "TEXT_OUTDIR_LOCAL=OUTDIR_LOCAL+'/revisions_txt/{}/'.format(LANGUAGE_CODE)\n",
    "TEXT_OUTDIR_lang_LOCAL = TEXT_OUTDIR_LOCAL+'/{}/'.format(LANGUAGE_CODE)\n",
    "TEXT_OUTDIR_en_LOCAL = TEXT_OUTDIR_LOCAL+'/{}/'.format('en')\n",
    "\n",
    "FEATURES_OUTDIR_LOCAL=OUTDIR_LOCAL+'/revisions_features/{}/'.format(LANGUAGE_CODE)\n",
    "FEATURES_OUTDIR_lang_LOCAL = FEATURES_OUTDIR_LOCAL+'/{}/'.format(LANGUAGE_CODE)\n",
    "FEATURES_OUTDIR_en_LOCAL = FEATURES_OUTDIR_LOCAL+'/{}/'.format('en')\n",
    "\n",
    "#get reliability related templates\n",
    "TEMPLATES_EN = [l.strip() for l in  open(TEMPLATES_FILE_EN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3dbd9a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -mkdir -p $outputHDFS $REVISIONS_OUTDIR $META_OUTDIR $PAIRS_OUTDIR $PAIRS_OUTDIR_lang $PAIRS_OUTDIR_en $TEXT_OUTDIR $FEATURES_OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2c54230",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p $OUTDIR_LOCAL $META_OUTDIR_LOCAL $PAIRS_OUTDIR_LOCAL $PAIRS_OUTDIR_lang_LOCAL $PAIRS_OUTDIR_en_LOCAL $TEXT_OUTDIR_LOCAL $TEXT_OUTDIR_lang_LOCAL $TEXT_OUTDIR_en_LOCAL $FEATURES_OUTDIR_LOCAL $FEATURES_OUTDIR_lang_LOCAL $FEATURES_OUTDIR_en_LOCAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460da92",
   "metadata": {},
   "source": [
    "#### CONNECT TO SESSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41ea368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PySpark executors will use /usr/lib/anaconda-wmf/bin/python3.\n"
     ]
    }
   ],
   "source": [
    "# mwapi session\n",
    "MWAPI_SESSION = mwapi.Session('https://en.wikipedia.org', user_agent='wikireliability -- kaywong@wikimedia')\n",
    "# get spark session\n",
    "spark = wmfdata.spark.get_session(type='yarn-large')\n",
    "from pyspark.sql.functions import udf,col, array\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import explode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6065bc8",
   "metadata": {},
   "source": [
    "## 1. Process revisions for templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d4bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langlink_templates(templates, langcode, mwapi_session):\n",
    "    \"\"\"\n",
    "    Get language linked title for list of `templates` from `langcode` wiki\n",
    "    \n",
    "    `templates`: list of templates to get language linked title\n",
    "    `langcode`: language code of wikiproject to search\n",
    "    \"\"\"\n",
    "    params = {'action':'query',\n",
    "              'prop':'langlinks',\n",
    "              'llprop': 'url|langname|autonym',\n",
    "              'lllang': langcode,\n",
    "              'format': 'json'\n",
    "             }\n",
    "    lang_templates = []\n",
    "\n",
    "    for template in templates:\n",
    "        params['titles'] = 'Template:{}'.format(template)\n",
    "        langlinks = mwapi_session.get(params)\n",
    "        langtitle = next(iter(langlinks['query']['pages'].values()))\n",
    "        langtitle = langtitle.get('langlinks', [{}])[0].get('*', None)\n",
    "        if langtitle:\n",
    "            langtitle=langtitle.split(':')[1].lower()\n",
    "        lang_templates.append(langtitle)\n",
    "\n",
    "    template_titles_map = dict(zip(templates, lang_templates))\n",
    "    templates_lang= [t for t in template_titles_map.values() if t!=None]\n",
    "    \n",
    "    return template_titles_map, templates_lang\n",
    "\n",
    "def getTemplatesRegexReliability(wikitext, templates):\n",
    "    \"\"\"\n",
    "    Extract list of templates from `wikitext` for an article via simple regex, \n",
    "    filtered by list of `templates`.\n",
    "    \n",
    "    `wikitext`: wikitext to parse for templates\n",
    "    `templates`: list of templates to filter by\n",
    "    \n",
    "    Known Issues:\n",
    "    * Doesn't handle nested templates (just gets the first)\n",
    "    -- e.g., '{{cite web|url=http://www.allmusic.com/|ref={{harvid|AllMusic}}}}' would be just web\n",
    "    \"\"\"\n",
    "    try:\n",
    "        all_templates = list(set([m.split('|')[0].strip() for m in re.findall('(?<=\\{\\{)(.*?)(?=\\}\\})', wikitext, flags=re.DOTALL)]))\n",
    "        all_templates = [str(t).strip().lower() for t in all_templates]\n",
    "        matching_templates = [template for template in all_templates if template in templates]\n",
    "        if len(matching_templates) > 0:\n",
    "            return matching_templates\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "775e2221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "Processing revision templates for ms\n",
      "LANG TEMPLATES: \n",
      "['pertikaian', 'tipu', 'pengesahan', 'tiada rujukan', 'penyelidikan asli', 'sumber tidak dipercayai', 'citation needed']\n",
      "EN TEMPLATES: \n",
      "['pov', 'disputed', 'third-party', 'contradict', 'hoax', 'more_citations_needed', 'unreferenced', 'original_research', 'unreliable_sources', 'one_source', 'citation_needed']\n",
      "ALL TEMPLATES: \n",
      "['more_citations_needed', 'pov', 'pengesahan', 'penyelidikan asli', 'third-party', 'unreliable_sources', 'citation needed', 'sumber tidak dipercayai', 'tiada rujukan', 'original_research', 'unreferenced', 'one_source', 'citation_needed', 'hoax', 'tipu', 'pertikaian', 'disputed', 'contradict']\n",
      "\n",
      "Get revisions containing reliability templates...\n",
      "+--------------------+-----+\n",
      "|                 col|count|\n",
      "+--------------------+-----+\n",
      "|     citation needed|26587|\n",
      "|        unreferenced|10249|\n",
      "|          pengesahan| 3391|\n",
      "|                 pov| 2080|\n",
      "|       tiada rujukan| 1265|\n",
      "|            disputed|  447|\n",
      "|   penyelidikan asli|  106|\n",
      "|         third-party|   94|\n",
      "|          pertikaian|   88|\n",
      "|sumber tidak dipe...|   15|\n",
      "|                hoax|    5|\n",
      "|          contradict|    4|\n",
      "+--------------------+-----+\n",
      "\n",
      "\n",
      "Writing to parquet file...\n",
      "Time taken:  89.75911617279053\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================================================\")\n",
    "print(\"Processing revision templates for {}\".format(LANGUAGE_CODE))\n",
    "\n",
    "t1 = time()        \n",
    "\n",
    "# Get language linked template titles\n",
    "template_titles_map, templates_lang= get_langlink_templates(TEMPLATES_EN, LANGUAGE_CODE, MWAPI_SESSION)\n",
    "templates_ALL= TEMPLATES_EN+templates_lang\n",
    "templates_ALL=list(set([t.lower() for t in templates_ALL]))\n",
    "print(\"LANG TEMPLATES: \\n{}\\nEN TEMPLATES: \\n{}\\nALL TEMPLATES: \\n{}\".format(templates_lang, TEMPLATES_EN, templates_ALL))\n",
    "\n",
    "print(\"\\nGet revisions containing reliability templates...\")\n",
    "# Get wikitexthistory for this language wiki\n",
    "wikidb = \"{}wiki\".format(LANGUAGE_CODE)\n",
    "wikitext_history = spark.sql('''SELECT page_id,revision_id,revision_text,page_namespace FROM wmf.mediawiki_wikitext_history \n",
    "    WHERE snapshot =\"{snapshot}\" and wiki_db =\"{wikidb}\"'''.format(wikidb=wikidb,snapshot=SNAPSHOT))\n",
    "\n",
    "# Get revisions containing the reliability templates\n",
    "# curry getTemplatesRegexReliability for specific templates\n",
    "getTemplatesRegexReliability_lang = udf(lambda wktext: getTemplatesRegexReliability(wktext, templates_ALL), \\\n",
    "                              returnType=ArrayType(StringType()) )\n",
    "wikitext_history = wikitext_history.withColumn(\"templates\",getTemplatesRegexReliability_lang(col('revision_text')))\n",
    "revisions_with_templates = wikitext_history.select(wikitext_history.page_id,wikitext_history.revision_id,explode(wikitext_history.templates))\n",
    "\n",
    "# Get template counts\n",
    "template_counts= revisions_with_templates.select('col').groupBy('col').count().orderBy('count', ascending=False)\n",
    "template_counts.show()\n",
    "template_counts.coalesce(1).write.mode('overwrite').option('header','true').csv(REVISIONS_OUTDIR+'{}_templatecounts.csv'.format(LANGUAGE_CODE))\n",
    "\n",
    "print(\"\\nWriting to parquet file...\")\n",
    "# Write to parquet file\n",
    "revisions_with_templates.write.parquet(REVISIONS_OUTDIR+'/{}_templates.parquet'.format(LANGUAGE_CODE),mode='overwrite')\n",
    "\n",
    "t2 = time()\n",
    "print('Time taken: ',t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3677a8",
   "metadata": {},
   "source": [
    "## 2. Get revision metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f5c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reverted(revisions_with_template, template, outf, localoutf): \n",
    "    \"\"\"\n",
    "    Get meta-info for revisions from the mediawiki_history table, w/ revertion information\n",
    "    \n",
    "    `revisions_with_template`: Spark dataframe of revisions\n",
    "    `template`: Template to process\n",
    "    `outf`: output file path on HDFS\n",
    "    `localoutf`: output file path on local machine\n",
    "    \"\"\"\n",
    "    t1 = time()\n",
    "    print(template)\n",
    "    df = revisions_with_template.where(revisions_with_template['col']==template)\n",
    "    df.cache()\n",
    "    t2 = time()\n",
    "    print('read table, done',t2-t1)\n",
    "    t1 = time()        \n",
    "    page_ids = df.select('page_id').distinct() # get all unique pages transcluding a template\n",
    "    page_ids.createOrReplaceTempView('tmp_page_ids')\n",
    "    revision_ids = df.select('revision_id').distinct()\n",
    "    revision_ids.createOrReplaceTempView('tmp_revision_ids')\n",
    "    reverts= spark.sql('''\n",
    "        SELECT w.event_timestamp, w.page_title,w.page_id, w.page_namespace,\n",
    "        w.revision_id, w.revision_is_identity_reverted, \n",
    "        w.revision_minor_edit, w.revision_text_bytes, \n",
    "        w.revision_first_identity_reverting_revision_id, w.revision_seconds_to_identity_revert,\n",
    "        w.event_user_id,w.event_user_registration_timestamp, \n",
    "        w.event_user_is_anonymous,w.event_user_revision_count,\n",
    "        CASE WHEN r.revision_id IS NOT NULL  THEN 1 ELSE 0 END has_template,\n",
    "        w.event_comment\n",
    "\n",
    "        FROM mediawiki_history_subset w LEFT OUTER JOIN tmp_revision_ids r \n",
    "                                    ON (w.revision_id = r.revision_id)\n",
    "\n",
    "        WHERE  w.page_id IN (\n",
    "                            SELECT  page_id FROM tmp_page_ids) \n",
    "        ORDER BY page_id, w.revision_id\n",
    "        ''') \n",
    "\n",
    "    if not reverts.rdd.isEmpty():\n",
    "        reverts.repartition(1).write.csv(outf,header=True,mode='overwrite',sep='\\t')\n",
    "        !hadoop fs -text \"$outf/*\"  | gzip -c > $localoutf-meta.csv.gz\n",
    "        print(\"Saved table\")\n",
    "    else: \n",
    "        print(\"empty df\")\n",
    "    t2 = time()\n",
    "    print('done',t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "720c1178",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================================\n",
      "Processing revision templates for ms\n",
      "LANG TEMPLATES: \n",
      "['pertikaian', 'tipu', 'pengesahan', 'tiada rujukan', 'penyelidikan asli', 'sumber tidak dipercayai', 'citation needed']\n",
      "EN TEMPLATES: \n",
      "['pov', 'disputed', 'third-party', 'contradict', 'hoax', 'more_citations_needed', 'unreferenced', 'original_research', 'unreliable_sources', 'one_source', 'citation_needed']\n",
      "\n",
      "Read in revisions file\n",
      "pertikaian\n",
      "read table, done 0.006998300552368164\n",
      "21/06/21 14:08:18 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 273.1199948787689\n",
      "tipu\n",
      "read table, done 0.0901646614074707\n",
      "empty df\n",
      "done 25.68256974220276\n",
      "pengesahan\n",
      "read table, done 0.07528209686279297\n",
      "21/06/21 14:09:18 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 34.868781089782715\n",
      "tiada rujukan\n",
      "read table, done 0.0652918815612793\n",
      "21/06/21 14:09:36 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 17.958820343017578\n",
      "penyelidikan asli\n",
      "read table, done 0.08086013793945312\n",
      "21/06/21 14:09:51 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 14.450023651123047\n",
      "sumber tidak dipercayai\n",
      "read table, done 0.06289124488830566\n",
      "21/06/21 14:10:04 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 13.4874107837677\n",
      "citation needed\n",
      "read table, done 0.06044912338256836\n",
      "21/06/21 14:10:32 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 27.895395278930664\n",
      "pov\n",
      "read table, done 0.09838557243347168\n",
      "21/06/21 14:10:52 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 19.923036575317383\n",
      "disputed\n",
      "read table, done 0.08056426048278809\n",
      "21/06/21 14:11:07 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 14.011658906936646\n",
      "third-party\n",
      "read table, done 0.07535839080810547\n",
      "21/06/21 14:11:22 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 15.094801664352417\n",
      "contradict\n",
      "read table, done 0.09363079071044922\n",
      "21/06/21 14:11:36 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 13.763751983642578\n",
      "hoax\n",
      "read table, done 0.06707477569580078\n",
      "21/06/21 14:11:48 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 12.327160596847534\n",
      "more_citations_needed\n",
      "read table, done 0.09140396118164062\n",
      "empty df\n",
      "done 4.406033277511597\n",
      "unreferenced\n",
      "read table, done 0.08142995834350586\n",
      "21/06/21 14:12:08 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved table\n",
      "done 16.133687496185303\n",
      "original_research\n",
      "read table, done 0.08550119400024414\n",
      "empty df\n",
      "done 4.069954872131348\n",
      "unreliable_sources\n",
      "read table, done 0.09345722198486328\n",
      "empty df\n",
      "done 4.541615724563599\n",
      "one_source\n",
      "read table, done 0.06717157363891602\n",
      "empty df\n",
      "done 5.808117151260376\n",
      "citation_needed\n",
      "read table, done 0.08869123458862305\n",
      "empty df\n",
      "done 6.254155158996582\n"
     ]
    }
   ],
   "source": [
    "print(\"==============================================================================================\")\n",
    "print(\"Processing revision templates for {}\".format(LANGUAGE_CODE))     \n",
    "\n",
    "# Get language linked template titles\n",
    "template_titles_map, templates_lang = get_langlink_templates(TEMPLATES_EN, LANGUAGE_CODE, MWAPI_SESSION)\n",
    "lang2en = {v: k for k, v in template_titles_map.items()}\n",
    "print(\"LANG TEMPLATES: \\n{}\\nEN TEMPLATES: \\n{}\\n\".format(templates_lang, TEMPLATES_EN))\n",
    "\n",
    "# Read in revisions_with_template file\n",
    "rev_inf=REVISIONS_OUTDIR+'/{}_templates.parquet'.format(LANGUAGE_CODE)\n",
    "print(\"Read in revisions file\")\n",
    "revisions_with_template = spark.read.parquet(rev_inf)\n",
    "revisions_with_template.cache()\n",
    "\n",
    "# Fet subset of pages from mwhistory which have >=1 occurence of reliability template\n",
    "pages_templates_subset = revisions_with_template.select('page_id').distinct()\n",
    "pages_templates_subset.createOrReplaceTempView('pages_templates_subset')\n",
    "\n",
    "mediawiki_history_subset =  spark.sql('''\n",
    "        SELECT w.event_timestamp, w.page_title,w.page_id,w.page_namespace, \n",
    "        w.revision_id, w.revision_is_identity_reverted, \n",
    "        w.revision_minor_edit, w.revision_text_bytes, \n",
    "        w.revision_first_identity_reverting_revision_id, w.revision_seconds_to_identity_revert,\n",
    "        w.event_user_id,w.event_user_registration_timestamp, \n",
    "        w.event_user_is_anonymous,w.event_user_revision_count,\n",
    "\n",
    "        w.event_comment\n",
    "        FROM wmf.mediawiki_history w\n",
    "        WHERE w.snapshot =\"{0}\" and w.wiki_db =\"{1}wiki\" AND  \n",
    "      w.event_entity = 'revision' AND w.page_id IN (\n",
    "                    SELECT  page_id FROM pages_templates_subset)                   \n",
    "        '''.format(SNAPSHOT, LANGUAGE_CODE))\n",
    "mediawiki_history_subset.cache()\n",
    "mediawiki_history_subset.createOrReplaceTempView('mediawiki_history_subset')\n",
    "\n",
    "# For lang version of templates\n",
    "for template in templates_lang:\n",
    "    try:\n",
    "        outf=META_OUTDIR+\"/{}-{}\".format(lang2en[template], LANGUAGE_CODE)\n",
    "        localoutf = META_OUTDIR_LOCAL+\"/{}-{}\".format(lang2en[template], LANGUAGE_CODE)\n",
    "        get_reverted(revisions_with_template, template, outf, localoutf)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('error',e)\n",
    "\n",
    "# For EN version of templates\n",
    "for template in TEMPLATES_EN:\n",
    "    try:\n",
    "        outf = META_OUTDIR+\"/{}-en\".format(template)\n",
    "        localoutf = META_OUTDIR_LOCAL+\"/{}-en\".format(template)\n",
    "        get_reverted(revisions_with_template, template, outf, localoutf)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('error',e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc693734",
   "metadata": {},
   "source": [
    "## 3. Get template addition/removal pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosNegRevisionPairs(inputF, outF, outFlocal):\n",
    "    '''\n",
    "    Get all positive/negative revision pairs of template addition and removal\n",
    "    \n",
    "    `inputF`: File path to revisions meta info\n",
    "    `outF`: output file path on HDFS\n",
    "    `outFlocal`: output file path on local machine\n",
    "    '''\n",
    "    \n",
    "    # Read input table\n",
    "    t1 = time()\n",
    "    revisions_meta = spark.read \\\n",
    "                     .option(\"header\", \"true\") \\\n",
    "                     .option(\"delimiter\", \"\\t\") \\\n",
    "                     .csv(inputF)\n",
    "    \n",
    "    t2 = time()\n",
    "    print('read table, done',t2-t1)\n",
    "\n",
    "    # Check table is not empty    \n",
    "    assert revisions_meta.rdd.isEmpty()==False, 'Empty table'\n",
    "\n",
    "    # Remove reverted revisions\n",
    "    t1 = time()\n",
    "    rev_unreverted = revisions_meta.where(revisions_meta['revision_is_identity_reverted']==False)\n",
    "    # Filter pages without has_template\n",
    "    pages_hastemplate_subset = rev_unreverted.where(rev_unreverted['has_template']==1).select('page_id').distinct()\n",
    "    pages_hastemplate_subset.createOrReplaceTempView('pages_hastemplate_subset')\n",
    "    rev_filtered = rev_unreverted.join(pages_hastemplate_subset,'page_id','inner')\n",
    "    rev_filtered = rev_filtered.select(col('page_id'), col('revision_id'), col('revision_minor_edit'), col('revision_text_bytes'), col('event_user_is_anonymous'), col('has_template'))\n",
    "    t2 = time()\n",
    "    print(\"\\t Remove reverted count: {}\".format(rev_unreverted.count()))\n",
    "    print(\"\\t Filtered pages count: {}\".format(rev_filtered.count()))\n",
    "    print('Filter reverted revisions, done',t2-t1)\n",
    "\n",
    "\n",
    "    # Extract positive and negative revision pairs\n",
    "    ## Gets all revisions with change in has template\n",
    "    rev_filtered.createOrReplaceTempView('rev_filtered')\n",
    "    t1 = time()\n",
    "    rev_posneg = spark.sql(\n",
    "        '''\n",
    "        SELECT r.page_id, r.revision_id, r.revision_minor_edit, r.revision_text_bytes, r.event_user_is_anonymous, r.has_template\n",
    "            FROM (SELECT *, \n",
    "                  LAG(has_template) OVER (PARTITION BY page_id ORDER BY CAST(revision_id AS INT)) as prev_has_template\n",
    "                  FROM rev_filtered) r\n",
    "        WHERE (has_template <> prev_has_template) OR (has_template=1 AND prev_has_template IS NULL)\n",
    "        '''\n",
    "    )\n",
    "\n",
    "    ## Filters groups of pages to select pairs\n",
    "    rev_posneg.createOrReplaceTempView('rev_posneg')\n",
    "    rev_posneg_filtered = spark.sql(\n",
    "        '''\n",
    "        SELECT *\n",
    "        FROM\n",
    "            (/* Select all pages with even number of revisions*/\n",
    "            SELECT R1.page_id, R1.revision_id, R1.revision_minor_edit, R1.revision_text_bytes, R1.event_user_is_anonymous, R1.has_template \n",
    "            FROM rev_posneg R1\n",
    "            INNER JOIN \n",
    "                (SELECT page_id\n",
    "                FROM rev_posneg\n",
    "                GROUP BY page_id\n",
    "                HAVING COUNT(page_id) % 2 = 0\n",
    "               ) R2\n",
    "            ON R1.page_id = R2.page_id)\n",
    "        UNION ALL\n",
    "            (/* Select all pages with odd number of revisions, but removing last row */\n",
    "            SELECT R3.page_id, R3.revision_id, R3.revision_minor_edit, R3.revision_text_bytes, R3.event_user_is_anonymous, R3.has_template\n",
    "                FROM (SELECT R4.page_id, R4.revision_id, R4.revision_minor_edit, R4.revision_text_bytes, R4.event_user_is_anonymous, R4.has_template, \n",
    "                      ROW_NUMBER() OVER (PARTITION BY R4.page_id ORDER BY CAST(R4.revision_id AS INT) DESC) AS GROUPEDROWNUM\n",
    "                  FROM rev_posneg R4\n",
    "                      INNER JOIN \n",
    "                      (SELECT page_id\n",
    "                       FROM rev_posneg\n",
    "                       GROUP BY page_id\n",
    "                       HAVING COUNT(page_id) % 2 <> 0\n",
    "                      ) R5\n",
    "                  ON R4.page_id = R5.page_id\n",
    "                ) R3    \n",
    "            WHERE R3.GROUPEDROWNUM >1\n",
    "            ORDER BY R3.page_id, CAST(R3.revision_id AS INT))\n",
    "        '''\n",
    "    )\n",
    "    t2 = time()\n",
    "    print('Extract template addition and removal pairs, done',t2-t1)\n",
    "\n",
    "    # Add revision_id pair ID\n",
    "    t1 = time()\n",
    "    rev_posneg_filtered.createOrReplaceTempView('rev_posneg_filtered')\n",
    "\n",
    "    rev_posneg_final = spark.sql(\n",
    "            '''\n",
    "            SELECT page_id, revision_id, revision_id_key, revision_minor_edit, revision_text_bytes, event_user_is_anonymous, has_template\n",
    "            FROM \n",
    "                (SELECT R1.page_id, R1.revision_id, R1.revision_id_key, R1.revision_minor_edit, R1.revision_text_bytes, R1.event_user_is_anonymous, R1.has_template\n",
    "                    FROM \n",
    "                        (SELECT *, \n",
    "                        LEAD(revision_id) OVER (ORDER BY page_id, CAST(revision_id AS INT)) AS revision_id_key,\n",
    "                        ROW_NUMBER() OVER (ORDER BY page_id, CAST(revision_id AS INT)) AS ROWNUM\n",
    "                        FROM rev_posneg_filtered) R1\n",
    "                    WHERE R1.ROWNUM % 2 <>0)\n",
    "                UNION ALL\n",
    "                    (SELECT R2.page_id, R2.revision_id, R2.revision_id_key, R2.revision_minor_edit, R2.revision_text_bytes, R2.event_user_is_anonymous, R2.has_template\n",
    "                    FROM \n",
    "                        (SELECT *, \n",
    "                        LAG(revision_id) OVER (ORDER BY page_id, CAST(revision_id AS INT)) AS revision_id_key,\n",
    "                        ROW_NUMBER() OVER (ORDER BY page_id, CAST(revision_id AS INT)) AS ROWNUM\n",
    "                        FROM rev_posneg_filtered) R2\n",
    "                    WHERE R2.ROWNUM % 2 =0) \n",
    "\n",
    "            ORDER BY page_id, CAST(revision_id AS INT)\n",
    "\n",
    "            '''\n",
    "        )\n",
    "    t2 = time()\n",
    "    print('Template pairs, done',t2-t1)\n",
    "    \n",
    "    # Remove empty pages\n",
    "    rev_posneg_final= rev_posneg_final.filter(~(col(\"txt_neg\")== \"\"))\n",
    "\n",
    "    t1 = time()\n",
    "    if not rev_posneg_final.rdd.isEmpty():\n",
    "        rev_posneg_final.repartition(1).write.csv(outF,header=True,mode='overwrite',sep='\\t')\n",
    "        !hadoop fs -text \"$outF/*\"  | gzip -c > $outFlocal-pairs.csv.gz\n",
    "        print('Saved to {}'.format(outF))\n",
    "    else:\n",
    "        print(\"EMPTY TABLE\")\n",
    "\n",
    "    t2 = time()        \n",
    "    print('done', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "50fa4c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get template addition/removal pairs for more_citations_needed, ms\n",
      "read table, done 1.8162105083465576\n",
      "\t Remove reverted count: 14763\n",
      "\t Filtered pages count: 14614\n",
      "Filter reverted revisions, done 0.07820963859558105\n",
      "Extract template addition and removal pairs, done 0.15042448043823242\n",
      "Template pairs, done 0.08742117881774902\n",
      "21/06/21 14:18:29 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//ms/more_citations_needed-ms-pairs.csv\n",
      "done 11.594306468963623\n",
      "\n",
      "Get template addition/removal pairs for original_research, ms\n",
      "read table, done 0.2519402503967285\n",
      "\t Remove reverted count: 1518\n",
      "\t Filtered pages count: 849\n",
      "Filter reverted revisions, done 0.05218029022216797\n",
      "Extract template addition and removal pairs, done 0.11555790901184082\n",
      "Template pairs, done 0.04957413673400879\n",
      "21/06/21 14:18:39 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//ms/original_research-ms-pairs.csv\n",
      "done 8.364187002182007\n",
      "\n",
      "Get template addition/removal pairs for citation_needed, ms\n",
      "read table, done 0.2590498924255371\n",
      "\t Remove reverted count: 56195\n",
      "\t Filtered pages count: 53268\n",
      "Filter reverted revisions, done 0.08267545700073242\n",
      "Extract template addition and removal pairs, done 0.11009335517883301\n",
      "Template pairs, done 0.04847311973571777\n",
      "21/06/21 14:18:54 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//ms/citation_needed-ms-pairs.csv\n",
      "done 13.065936088562012\n",
      "\n",
      "Get template addition/removal pairs for hoax, en\n",
      "read table, done 0.2751893997192383\n",
      "\t Remove reverted count: 72\n",
      "\t Filtered pages count: 72\n",
      "Filter reverted revisions, done 0.07788515090942383\n",
      "Extract template addition and removal pairs, done 0.12816691398620605\n",
      "Template pairs, done 0.05190396308898926\n",
      "21/06/21 14:19:05 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//en/hoax-en-pairs.csv\n",
      "done 9.2587149143219\n",
      "\n",
      "Get template addition/removal pairs for unreferenced, ms\n",
      "read table, done 0.3180723190307617\n",
      "\t Remove reverted count: 9323\n",
      "\t Filtered pages count: 7704\n",
      "Filter reverted revisions, done 0.0877389907836914\n",
      "Extract template addition and removal pairs, done 0.0910181999206543\n",
      "Template pairs, done 0.05248451232910156\n",
      "21/06/21 14:19:22 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//ms/unreferenced-ms-pairs.csv\n",
      "done 14.090070962905884\n",
      "\n",
      "Get template addition/removal pairs for contradict, en\n",
      "read table, done 0.28144001960754395\n",
      "\t Remove reverted count: 4\n",
      "\t Filtered pages count: 4\n",
      "Filter reverted revisions, done 0.052182912826538086\n",
      "Extract template addition and removal pairs, done 0.12344169616699219\n",
      "Template pairs, done 0.05427694320678711\n",
      "EMPTY TABLE\n",
      "done 3.1913394927978516\n",
      "\n",
      "Get template addition/removal pairs for unreliable_sources, ms\n",
      "read table, done 0.23462939262390137\n",
      "\t Remove reverted count: 509\n",
      "\t Filtered pages count: 137\n",
      "Filter reverted revisions, done 0.14219236373901367\n",
      "Extract template addition and removal pairs, done 0.12479782104492188\n",
      "Template pairs, done 0.0501406192779541\n",
      "21/06/21 14:19:39 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//ms/unreliable_sources-ms-pairs.csv\n",
      "done 10.165770530700684\n",
      "\n",
      "Get template addition/removal pairs for third-party, en\n",
      "read table, done 0.2699012756347656\n",
      "\t Remove reverted count: 826\n",
      "\t Filtered pages count: 826\n",
      "Filter reverted revisions, done 0.06498837471008301\n",
      "Extract template addition and removal pairs, done 0.11754465103149414\n",
      "Template pairs, done 0.048918724060058594\n",
      "EMPTY TABLE\n",
      "done 4.099079370498657\n",
      "\n",
      "Get template addition/removal pairs for unreferenced, en\n",
      "read table, done 0.28832530975341797\n",
      "\t Remove reverted count: 43659\n",
      "\t Filtered pages count: 42374\n",
      "Filter reverted revisions, done 0.08177781105041504\n",
      "Extract template addition and removal pairs, done 0.12568449974060059\n",
      "Template pairs, done 0.04953455924987793\n",
      "21/06/21 14:20:04 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//en/unreferenced-en-pairs.csv\n",
      "done 15.994677782058716\n",
      "\n",
      "Get template addition/removal pairs for disputed, ms\n",
      "read table, done 0.2616088390350342\n",
      "\t Remove reverted count: 514\n",
      "\t Filtered pages count: 514\n",
      "Filter reverted revisions, done 0.08157515525817871\n",
      "Extract template addition and removal pairs, done 0.10655045509338379\n",
      "Template pairs, done 0.052190303802490234\n",
      "21/06/21 14:20:18 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//ms/disputed-ms-pairs.csv\n",
      "done 12.007309436798096\n",
      "\n",
      "Get template addition/removal pairs for disputed, en\n",
      "read table, done 0.2949485778808594\n",
      "\t Remove reverted count: 1803\n",
      "\t Filtered pages count: 1769\n",
      "Filter reverted revisions, done 0.08880376815795898\n",
      "Extract template addition and removal pairs, done 0.12619733810424805\n",
      "Template pairs, done 0.05133366584777832\n",
      "21/06/21 14:20:31 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//en/disputed-en-pairs.csv\n",
      "done 11.067365407943726\n",
      "\n",
      "Get template addition/removal pairs for pov, en\n",
      "read table, done 0.3230419158935547\n",
      "\t Remove reverted count: 6101\n",
      "\t Filtered pages count: 5300\n",
      "Filter reverted revisions, done 0.12340521812438965\n",
      "Extract template addition and removal pairs, done 0.12300252914428711\n",
      "Template pairs, done 0.0695810317993164\n",
      "21/06/21 14:20:46 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Saved to ./HDFSout//revisions_pairs/ms//en/pov-en-pairs.csv\n",
      "done 12.61293363571167\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE_FNAMES = glob.glob(META_OUTDIR_LOCAL+'/*.csv.gz')\n",
    "\n",
    "for template_fname in TEMPLATE_FNAMES:\n",
    "    searchstr = \"(.*)-({0}|en)-meta.csv.gz\".format(LANGUAGE_CODE)\n",
    "    template_fname = template_fname.split('/')[-1]\n",
    "    template_match = re.match(searchstr, template_fname)\n",
    "    template = template_match[1] if template_match else None\n",
    "    template_lang = template_match[2] if template_match else None\n",
    "    \n",
    "    inputF = META_OUTDIR+\"/{0}-{1}\".format(template, template_lang)\n",
    "    outF = PAIRS_OUTDIR+'/{0}/{1}-{0}-pairs.csv'.format(template_lang, template)\n",
    "    outFlocal = PAIRS_OUTDIR_LOCAL+'/{0}/{1}-{0}'.format(template_lang, template)\n",
    "    \n",
    "    print(\"\\nGet template addition/removal pairs for {}, {}\".format(template, template_lang))\n",
    "    try:\n",
    "        getPosNegRevisionPairs(inputF, outF, outFlocal) \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6334c",
   "metadata": {},
   "source": [
    "## 4. Get revision text and diff text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3407a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def getContentText(wikicode):\n",
    "    \"\"\"\n",
    "    Parse wikicode for plain content text\n",
    "    \n",
    "    `wikicode`: wikicode/revision text to parse\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed_wc = mwp.parse(wikicode or \"\")\n",
    "    sections = parsed_wc.get_sections()\n",
    "    sections = [section.strip_code().strip() for section in sections]\n",
    "    filtered_sections= [section for section in sections if not section.startswith((\"See also\", \"References\", \"External links\", \"Footnotes\",\"Further reading\", \"Bibliography\" ))]\n",
    "    content_txt = \"\\n\".join(filtered_sections)\n",
    "        \n",
    "    return content_txt\n",
    "\n",
    "def getDiffText(pos_txt, neg_txt):\n",
    "    \"\"\"\n",
    "    Get diff'd sections between two versions of text\n",
    "    \n",
    "    `pos_txt`: Positive instance of template addition\n",
    "    `neg_txt`: Negative instance of template removal\n",
    "    \"\"\"\n",
    "    diff_pos=[]; diff_neg=[]\n",
    "\n",
    "    for l in difflib.ndiff(pos_txt.splitlines(), neg_txt.splitlines()):\n",
    "        if l[0] =='-':\n",
    "            diff_pos.append(l)\n",
    "        elif l[0]=='+':\n",
    "            diff_neg.append(l)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    difftxt_pos=\" \".join([l[2:] for l in diff_pos])\n",
    "    difftxt_neg=\" \".join([l[2:] for l in diff_neg])\n",
    "    \n",
    "    return (difftxt_pos, difftxt_neg)\n",
    "\n",
    "# Instantiate getDiffText UDF which returns multiple fields\n",
    "schema = StructType([\n",
    "    StructField(\"difftxt_pos\", StringType(), False),\n",
    "    StructField(\"difftxt_neg\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "getDiffText_udf = udf(getDiffText, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac48123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosNegTextPairs(inputF, outputF, outFlocal):\n",
    "    \"\"\"\n",
    "    Processes posneg text pairs for `inputF` and writes output to JSON\n",
    "    \n",
    "    `inputF`: input file name of pairs data\n",
    "    `outputF`: output filepath on HDFS\n",
    "    `outFlocal`: output filepath local\n",
    "    \"\"\"\n",
    "    # Get rev posneg pairs\n",
    "    rev_posneg = spark.read \\\n",
    "                     .option(\"header\", \"true\") \\\n",
    "                     .option(\"delimiter\", \"\\t\") \\\n",
    "                     .csv(inputF)\n",
    "\n",
    "    # Get wikitexthistory for this language wiki\n",
    "    wikidb = \"{}wiki\".format(LANGUAGE_CODE)\n",
    "    wikitext_history = spark.sql('''SELECT page_id,revision_id,revision_text,page_namespace FROM wmf.mediawiki_wikitext_history \n",
    "        WHERE snapshot =\"{snapshot}\" and wiki_db =\"{wikidb}\"'''.format(wikidb=wikidb,snapshot=SNAPSHOT))\n",
    "\n",
    "    # Join on both to get revtext\n",
    "    rev_posneg_txt = rev_posneg.join(wikitext_history,['page_id', 'revision_id'],'left')\n",
    "\n",
    "    # Parse wikitext for plain content text\n",
    "    rev_posneg_txt = rev_posneg_txt.withColumn(\"txt\", getContentText(col('revision_text')))\n",
    "    rev_posneg_txt = rev_posneg_txt.select(col('page_id'), col('revision_id'), col('revision_id_key'), col('has_template'), col('txt'))\n",
    "\n",
    "    # Join posneg pair entries as a single row\n",
    "    rev_pos_txt = rev_posneg_txt.where(rev_posneg_txt['has_template']==1)\n",
    "    rev_pos_txt = rev_pos_txt.withColumnRenamed(\"revision_id\", \"revision_id_pos\")\\\n",
    "                             .withColumnRenamed(\"revision_id_key\", \"revision_id_neg\")\\\n",
    "                             .withColumnRenamed(\"txt\", \"txt_pos\")\n",
    "\n",
    "    rev_neg_txt = rev_posneg_txt.where(rev_posneg_txt['has_template']==0)\n",
    "    rev_neg_txt = rev_neg_txt.withColumnRenamed(\"revision_id\", \"revision_id_neg\")\\\n",
    "                             .withColumnRenamed(\"revision_id_key\", \"revision_id_pos\")\\\n",
    "                             .withColumnRenamed(\"txt\", \"txt_neg\")\n",
    "\n",
    "    rev_pairs_txt = rev_pos_txt.join(rev_neg_txt, ['page_id', 'revision_id_pos', 'revision_id_neg'], 'inner')\n",
    "    rev_pairs_txt = rev_pairs_txt.select(col('page_id'), col('revision_id_pos'), col('revision_id_neg'), col('txt_pos'), col('txt_neg'))\n",
    "        \n",
    "    # Write txt to output file\n",
    "    rev_pairs_txt.repartition(1).write.format('json').save(outputF, mode='overwrite')\n",
    "    !hadoop fs -text \"$outputF/*\"  | gzip -c > $outFlocal-txt.json.gz\n",
    "\n",
    "    # Get diff'd text sections between pos and neg text and write to output file   \n",
    "    rev_pairs_difftxt = rev_pairs_txt.withColumn(\"diffResults\", getDiffText_udf(rev_pairs_txt.txt_pos, rev_pairs_txt.txt_neg))\n",
    "    diff_outputF = outputF.replace(\"txt\", \"difftxt\")\n",
    "    rev_pairs_difftxt = rev_pairs_difftxt.select(col('page_id'), col('revision_id_pos'), col('revision_id_neg'), col('diffResults.*'))\n",
    "    rev_pairs_difftxt.repartition(1).write.format('json').save(diff_outputF, mode='overwrite')\n",
    "    !hadoop fs -text \"$diff_outputF/*\"  | gzip -c > $outFlocal-difftxt.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "725232cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//unreliable_sources-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/unreliable_sources-idid-txt.json\n",
      "21/07/01 07:39:04 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:40:02 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//one_source-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/one_source-idid-txt.json\n",
      "21/07/01 07:40:44 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:41:28 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//citation_needed-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/citation_needed-idid-txt.json\n",
      "21/07/01 07:42:23 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:43:55 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//unreferenced-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/unreferenced-idid-txt.json\n",
      "21/07/01 07:45:54 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:48:58 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//more_citations_needed-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/more_citations_needed-idid-txt.json\n",
      "21/07/01 07:50:42 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:51:33 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//disputed-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/disputed-idid-txt.json\n",
      "21/07/01 07:52:25 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:53:27 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//third-party-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/third-party-idid-txt.json\n",
      "21/07/01 07:54:18 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:55:07 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//original_research-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/original_research-idid-txt.json\n",
      "21/07/01 07:55:56 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:56:54 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//hoax-idid-pairs.csv, ./HDFSout//revisions_txt/id/id/hoax-idid-txt.json\n",
      "21/07/01 07:57:45 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 07:58:35 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//en//third-party-iden-pairs.csv, ./HDFSout//revisions_txt/id/en/third-party-iden-txt.json\n",
      "21/07/01 07:59:27 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 08:00:17 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//en//pov-iden-pairs.csv, ./HDFSout//revisions_txt/id/en/pov-iden-txt.json\n",
      "21/07/01 08:01:15 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 08:02:10 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//en//contradict-iden-pairs.csv, ./HDFSout//revisions_txt/id/en/contradict-iden-txt.json\n",
      "21/07/01 08:03:04 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 08:03:51 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//en//hoax-iden-pairs.csv, ./HDFSout//revisions_txt/id/en/hoax-iden-txt.json\n",
      "21/07/01 08:04:41 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 08:05:32 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//en//disputed-iden-pairs.csv, ./HDFSout//revisions_txt/id/en/disputed-iden-txt.json\n",
      "21/07/01 08:06:22 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 08:07:13 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get text for template addition/removal pairs ./HDFSout//revisions_pairs/id//en//unreferenced-iden-pairs.csv, ./HDFSout//revisions_txt/id/en/unreferenced-iden-txt.json\n",
      "21/07/01 08:08:31 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/07/01 08:10:17 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n"
     ]
    }
   ],
   "source": [
    "searchstr = \"(.*)-({0}|en)+\\-pairs.csv\".format(LANGUAGE_CODE)\n",
    "\n",
    "# Language version of template\n",
    "TEMPLATE_FNAMES_lang = glob.glob(PAIRS_OUTDIR_lang_LOCAL+'/*pairs.csv.gz')\n",
    "template_lang =LANGUAGE_CODE\n",
    "\n",
    "for template_fname in TEMPLATE_FNAMES_lang:\n",
    "    template_fname = template_fname.split('/')[-1]\n",
    "    template_match = re.match(searchstr, template_fname)\n",
    "    template = template_match[1] if template_match else None\n",
    "    \n",
    "    inputF = PAIRS_OUTDIR_lang+\"/{0}-{1}{2}-pairs.csv\".format(template, LANGUAGE_CODE, template_lang)\n",
    "    outputF = TEXT_OUTDIR+'{0}/{1}-{2}{0}-txt.json'.format(template_lang, template,LANGUAGE_CODE)\n",
    "    outFlocal = TEXT_OUTDIR_LOCAL+'{0}/{1}-{2}{0}'.format(template_lang, template, LANGUAGE_CODE)\n",
    "    print(\"Get text for template addition/removal pairs {}, {}\".format(inputF, outputF, outFlocal))\n",
    "    getPosNegTextPairs(inputF, outputF, outFlocal)\n",
    "    \n",
    "# En version of template\n",
    "TEMPLATE_FNAMES_en = glob.glob(PAIRS_OUTDIR_en_LOCAL+'/*pairs.csv.gz')\n",
    "template_lang ='en'\n",
    "for template_fname in TEMPLATE_FNAMES_en:\n",
    "    template_fname = template_fname.split('/')[-1]\n",
    "    template_match = re.match(searchstr, template_fname)\n",
    "    template = template_match[1] if template_match else None\n",
    "\n",
    "    inputF = PAIRS_OUTDIR_en+\"/{0}-{1}{2}-pairs.csv\".format(template, LANGUAGE_CODE, template_lang)\n",
    "    outputF = TEXT_OUTDIR+'{0}/{1}-{2}{0}-txt.json'.format(template_lang, template,LANGUAGE_CODE)\n",
    "    outFlocal = TEXT_OUTDIR_LOCAL+'{0}/{1}-{2}{0}'.format(template_lang, template, LANGUAGE_CODE)\n",
    "    \n",
    "    print(\"Get text for template addition/removal pairs {}, {}\".format(inputF, outputF, outFlocal))\n",
    "    getPosNegTextPairs(inputF, outputF, outFlocal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1113527e",
   "metadata": {},
   "source": [
    "## 5. Process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7bb07144",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfn_templates = [t.lower() for t in [\"Shortened footnote template\", \"sfn\", \"Sfnp\", \"Sfnm\", \"Sfnmp\"]]\n",
    "\n",
    "#@udf(returnType=IntegerType())\n",
    "def getNumRefs(wc):\n",
    "    \"\"\"\n",
    "    Extract list of links from wikitext for an article via mwparserfromhell.\n",
    "    `wc`: Wikicode parsed by mwparser\n",
    "    \"\"\"  \n",
    "    try:\n",
    "        #wc = mwparserfromhell.parse(wikitext)\n",
    "        num_ref_tags = len([t.tag for t in wc.filter_tags() if t.tag == 'ref'])\n",
    "        num_sftn_templates = len([t.name for t in wc.filter_templates() if t.name.lower() in sfn_templates])\n",
    "        return num_ref_tags + num_sftn_templates\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "#@udf(returnType=IntegerType())\n",
    "def getNumHeadings(wc, max_level=None):\n",
    "    \"\"\"\n",
    "    Extract list of headings from wikitext for an article.\n",
    "    `wc`: Wikicode parsed by mwparser\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #wc = mwparserfromhell.parse(wikitext)\n",
    "        if max_level is None:\n",
    "            return len([1 for l in wc.filter_headings()])\n",
    "        else:\n",
    "            return len([1 for l in wc.filter_headings() if l.level <= max_level])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def getImageNamespaceAlias(LANGUAGE_CODE):\n",
    "    \"\"\"\n",
    "    Get aliases for \"Image\" namespace in a langwiki\n",
    "    `LANGUAGE_CODE`: Language code of langwiki\n",
    "    \"\"\"\n",
    "    \n",
    "    S = requests.Session()\n",
    "    URL = \"https://{}.wikipedia.org/w/api.php\".format(LANGUAGE_CODE)\n",
    "    params = {'action':'query',\n",
    "              'meta':'siteinfo',\n",
    "              'siprop':'namespacealiases|namespaces',\n",
    "              'format': 'json'\n",
    "             }\n",
    "    R = S.get(url=URL, params=params)\n",
    "    DATA = R.json()\n",
    "    \n",
    "    # Get aliases for \"Image\" namespace in this langwiki\n",
    "    image_ns_alias = [alias['*'] for alias in DATA['query']['namespacealiases'] if alias['id']==6 ]\n",
    "    image_ns_alias.append(\"File\")\n",
    "    image_ns_alias = tuple(['[['+alias+':' for alias in image_ns_alias])\n",
    "    \n",
    "    return image_ns_alias\n",
    "\n",
    "def getNumImages(wc, image_ns_alias):    \n",
    "    \"\"\"\n",
    "    Extract number of images from an article\n",
    "    `wc`: Wikicode parsed by mwparser\n",
    "    `image_ns_alias`: tuple of namespace aliases for \"Image\" for specific languagewiki\n",
    "    \"\"\"\n",
    "    images = [l for l in wc.filter_wikilinks() if l.startswith(image_ns_alias)]\n",
    "    if images:\n",
    "        print(images)\n",
    "    return len(images)\n",
    "\n",
    "def getRevisionFeatures(wikitext, LANGUAGE_CODE):\n",
    "    \"\"\"\n",
    "    Extract revision features for a single revision wikitext\n",
    "    `wiktext`: Wikitext of revision\n",
    "    `LANGUAGE_CODE`: language code of languagewiki\n",
    "    \"\"\"\n",
    "\n",
    "    wc = mwp.parse(wikitext)\n",
    "    \n",
    "    if wc: \n",
    "        page_len = len(wikitext)  #normalise??\n",
    "        num_refs = getNumRefs(wc)\n",
    "        num_headings = getNumHeadings(wc)\n",
    "        image_ns_alias = getImageNamespaceAlias(LANGUAGE_CODE)\n",
    "        num_images = getNumImages(wc, image_ns_alias)\n",
    "    else:\n",
    "        page_len=0; num_refs=0; num_headings=0; num_images=0\n",
    "        \n",
    "    return (page_len, num_refs, num_headings, num_images)\n",
    "\n",
    "# Instantiate getDiffText UDF which returns multiple fields\n",
    "rev_features_schema = StructType([\n",
    "    StructField(\"page_len\", IntegerType(), False),\n",
    "    StructField(\"num_refs\", IntegerType(), False),\n",
    "    StructField(\"num_headings\", IntegerType(), False),\n",
    "    StructField(\"num_images\", IntegerType(), False)\n",
    "    ])\n",
    "\n",
    "getRevisionFeatures_udf = udf(lambda wktext:getRevisionFeatures(wktext, LANGUAGE_CODE), \\\n",
    "                              rev_features_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "373782a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosNegFeatures(inputF,outputF, outFlocal):\n",
    "    \"\"\"\n",
    "    Processes posneg feature pairs for `inputF` and writes output to CSV\n",
    "    \n",
    "    `inputF`: input file name of pairs data\n",
    "    `outputF`: output filepath on HDFS\n",
    "    `outFlocal`: output filepath local\n",
    "    \"\"\"\n",
    "    # Get rev posneg pairs\n",
    "    rev_posneg = spark.read \\\n",
    "                     .option(\"header\", \"true\") \\\n",
    "                     .option(\"delimiter\", \"\\t\") \\\n",
    "                     .csv(inputF)\n",
    "\n",
    "    # Get wikitexthistory for this language wiki\n",
    "    wikidb = \"{}wiki\".format(LANGUAGE_CODE)\n",
    "    wikitext_history = spark.sql('''SELECT page_id,revision_id,revision_text,page_namespace FROM wmf.mediawiki_wikitext_history \n",
    "        WHERE snapshot =\"{snapshot}\" and wiki_db =\"{wikidb}\"'''.format(wikidb=wikidb,snapshot=SNAPSHOT))\n",
    "    rev_posneg = rev_posneg.join(wikitext_history,['page_id','revision_id'],'left')\n",
    "\n",
    "    # Get language agnostic metadata features\n",
    "    rev_posneg_features = rev_posneg.withColumn(\"revFeatures\", getRevisionFeatures_udf(rev_posneg.revision_text))\n",
    "    rev_posneg_features_expanded = rev_posneg_features.select(col('page_id'), col('revision_id'), col('revision_id_key'), col('has_template'), col('revFeatures.*'))\n",
    "\n",
    "    # Write\n",
    "    rev_posneg_features_expanded.repartition(1).write.csv(outputF,header=True,mode='overwrite',sep='\\t')\n",
    "    !hadoop fs -text \"$outputF/*\"  | gzip -c > $outFlocal-features.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1485800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get features for template addition/removal pairs ./HDFSout//revisions_pairs/id//id//unreliable_sources-idid-pairs.csv, ./HDFSout//revisions_features/id/id/unreliable_sources-idid-features.csv\n",
      "21/07/24 05:03:18 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "Get features for template addition/removal pairs ./HDFSout//revisions_pairs/id//en//third-party-iden-pairs.csv, ./HDFSout//revisions_features/id/en/third-party-iden-features.csv\n",
      "21/07/24 05:03:56 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n"
     ]
    }
   ],
   "source": [
    "searchstr = \"(.*)-({0}|en)+\\-pairs.csv\".format(LANGUAGE_CODE)\n",
    "\n",
    "# Language version of template\n",
    "TEMPLATE_FNAMES_lang = glob.glob(PAIRS_OUTDIR_lang_LOCAL+'/*pairs.csv.gz')\n",
    "template_lang =LANGUAGE_CODE\n",
    "\n",
    "for template_fname in TEMPLATE_FNAMES_lang[:1]:\n",
    "    template_fname = template_fname.split('/')[-1]\n",
    "    template_match = re.match(searchstr, template_fname)\n",
    "    template = template_match[1] if template_match else None\n",
    "    \n",
    "    inputF = PAIRS_OUTDIR_lang+\"/{0}-{1}{2}-pairs.csv\".format(template, LANGUAGE_CODE, template_lang)\n",
    "    outputF = FEATURES_OUTDIR+'{0}/{1}-{2}{0}-features.csv'.format(template_lang, template,LANGUAGE_CODE)\n",
    "    outFlocal = FEATURES_OUTDIR_LOCAL+'{0}/{1}-{2}{0}'.format(template_lang, template, LANGUAGE_CODE)\n",
    "    print(\"Get features for template addition/removal pairs {}, {}\".format(inputF, outputF, outFlocal))\n",
    "    getPosNegFeatures(inputF, outputF, outFlocal)\n",
    "    \n",
    "# En version of template\n",
    "TEMPLATE_FNAMES_en = glob.glob(PAIRS_OUTDIR_en_LOCAL+'/*pairs.csv.gz')\n",
    "template_lang ='en'\n",
    "for template_fname in TEMPLATE_FNAMES_en[:1]:\n",
    "    template_fname = template_fname.split('/')[-1]\n",
    "    template_match = re.match(searchstr, template_fname)\n",
    "    template = template_match[1] if template_match else None\n",
    "\n",
    "    inputF = PAIRS_OUTDIR_en+\"/{0}-{1}{2}-pairs.csv\".format(template, LANGUAGE_CODE, template_lang)\n",
    "    outputF = FEATURES_OUTDIR+'{0}/{1}-{2}{0}-features.csv'.format(template_lang, template,LANGUAGE_CODE)\n",
    "    outFlocal = FEATURES_OUTDIR_LOCAL+'{0}/{1}-{2}{0}'.format(template_lang, template, LANGUAGE_CODE)\n",
    "    \n",
    "    print(\"Get features for template addition/removal pairs {}, {}\".format(inputF, outputF, outFlocal))\n",
    "    getPosNegFeatures(inputF, outputF, outFlocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5f45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
