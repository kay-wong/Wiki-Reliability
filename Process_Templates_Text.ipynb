{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting revision text from templates data\n",
    "\n",
    "Following from the `Process_Templates` notebook, which extracts datasets of template pairs and features, this notebook extracts the revision text data for each revision in the templates dataset to create a complementary datasett for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mwparserfromhell as mwp\n",
    "import gzip\n",
    "import csv\n",
    "from math import ceil\n",
    "import difflib\n",
    "import mwapi  \n",
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revision_text(revid, pageid):\n",
    "    \"\"\"\n",
    "    Queries a revision for its text\n",
    "    pageid: ID of page to query\n",
    "    revisionid: ID of revision to query\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"revids\": revid,\n",
    "        #\"rvprop\": \"content|timestamp|tags\"\n",
    "        \"rvprop\": \"content\"\n",
    "    }\n",
    "    res = session.get(params)  \n",
    "    try:\n",
    "        revision = res['query']['pages'][str(pageid)]['revisions'][0]\n",
    "        revision_txt = revision['*']\n",
    "    except KeyError:\n",
    "        return \"\"\n",
    "    \n",
    "    return revision_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_revision_text_chunked(revision_ids):\n",
    "    \"\"\"\n",
    "    Queries revisions for its text, chunked across multiple API calls based on API limit\n",
    "    revision_ids: IDs of revisions to query\n",
    "    \"\"\"\n",
    "    revid_chunks = [revision_ids[i:i + 50] for i in range(0, len(revision_ids),50)] \n",
    "    full_res = []\n",
    "    \n",
    "    with tqdm(total=len(revid_chunks)) as progbar:\n",
    "        for revid_chunk in revid_chunks:\n",
    "            params = {\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"revisions\",\n",
    "                \"revids\": \"|\".join(revid_chunk),\n",
    "                \"rvprop\": \"content|ids\"\n",
    "            }    \n",
    "\n",
    "            res = session.get(params)  \n",
    "            for _, page in res['query']['pages'].items():  \n",
    "                try:\n",
    "                    revs = pd.json_normalize(page['revisions'])\n",
    "                    revs.fillna(value={'*': \"\"}, inplace=True)\n",
    "                    revs['txt'] = revs['*'].apply(lambda x: parse_wikitext(x))\n",
    "                    revs = revs[['revid', 'parentid', 'txt']]\n",
    "\n",
    "                    full_res.extend(revs.to_dict('records'))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "\n",
    "            progbar.update(1)\n",
    "    \n",
    "    return full_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wikitext(wikicode):\n",
    "    \"\"\"\n",
    "    Parse wikitext for plain content text\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed_wc = mwp.parse(wikicode or \"\")\n",
    "    sections = parsed_wc.get_sections()\n",
    "    sections = [section.strip_code().strip() for section in sections]\n",
    "    filtered_sections= [section for section in sections if not section.startswith((\"See also\", \"References\", \"External links\", \"Footnotes\",\"Further reading\", \"Bibliography\" ))]\n",
    "    content_txt = \"\\n\".join(filtered_sections)\n",
    "        \n",
    "    return content_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_revision_text(revtext):\n",
    "    props={}\n",
    "\n",
    "    wc = mwp.parse(revtext or \"\")\n",
    "    extlinks = wc.filter_external_links()\n",
    "    props['extlinks_all'] = str([l.url for l in extlinks])\n",
    "    wikilinks = wc.filter_wikilinks()\n",
    "    props['wikilinks_all']= str([l.title for l in wikilinks])\n",
    "    headings = wc.filter_headings()\n",
    "    props['headings_all'] = str([(h.title, h.level) for h in headings])\n",
    "    templates = wc.filter_templates()\n",
    "    templates = list(set([t.name.strip().lower() for t in templates]))\n",
    "    props['templates_all'] = str(templates)\n",
    "    \n",
    "    return props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET REVISION TEXT DATA\n",
    "\n",
    "Query the Wikimedia API for revision text for each revision in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Wikimedia API session\n",
    "LANGUAGE = 'enwiki'\n",
    "SITENAME = LANGUAGE.replace('wiki', '.wikipedia')\n",
    "\n",
    "session = mwapi.Session('https://{0}.org'.format(SITENAME), user_agent='{0} -- {1}'.format('Outreachy Templates (mwapi)', '0xkaywong'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "TEMPLATES_DIR = 'OUTPUT-FOLDER' #Parent path\n",
    "TEMPLATE_FEATURES_DIR = TEMPLATES_DIR+'/features' #Same as `Process Templates` notebook\n",
    "TEMPLATE_TEXT_DIR = TEMPLATES_DIR+'/fulltext'\n",
    "TEMPLATE_DIFFTEXT_DIR = TEMPLATES_DIR+'/difftext'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 templates \n",
    "TEMPLATES_LIST = ['more_citations_needed', 'unreliable_sources', 'disputed', 'pov', 'third-party', 'contradict', 'hoax', 'one_source', 'unreferenced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get revision text for all templates\n",
    "for TEMPLATE_NAME in TEMPLATES_LIST:\n",
    "    print(\"========================================\")\n",
    "    print(TEMPLATE_NAME)\n",
    "    fname=TEMPLATES_FEATURES_DIR+'/{}_features.csv'.format(TEMPLATE_NAME)\n",
    "    full_df= pd.read_csv(fname, index_col=0)\n",
    "    print(len(full_df))\n",
    "\n",
    "    # Query\n",
    "    sample = full_df[['page_id', 'revision_id', 'has_template']]\n",
    "    txt_df = get_revision_text_chunked(sample.revision_id.astype(str))\n",
    "    txt_df = pd.DataFrame(txt_df)\n",
    "\n",
    "    txt_df = sample.merge(txt_df, left_on='revision_id', right_on='revid')\n",
    "    txt_df = txt_df[['page_id', 'revision_id', 'has_template', 'txt']]\n",
    "    outcsv_fname= TEMPLATE_TEXT_DIR+'/{}_fulltxt.csv.gz'.format(TEMPLATE_NAME)\n",
    "    txt_df.to_csv(outcsv_fname, compression='gzip')\n",
    "    print(\"Wrote to: {}\".format(outcsv_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET DIFF TEXT DATA\n",
    "\n",
    "In addition to the full revision text data, we further process this to obtain a shortened form based on only the changed sections between corresponding revision pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posneg_df(TEMPLATE_NAME):\n",
    "    fname=TEMPLATES_FEATURES_DIR+'/{}_features.csv'.format(TEMPLATE_NAME)\n",
    "    txt_fname= TEMPLATE_TEXT_DIR+'/{}_fulltxt.csv.gz'.format(TEMPLATE_NAME)\n",
    "    \n",
    "    full_df= pd.read_csv(fname, index_col=0)\n",
    "    \n",
    "    pos_df = full_df[full_df.has_template==1]\n",
    "    neg_df = full_df[full_df.has_template==0]\n",
    "    pos_txt_df = pos_df[['page_id', 'revision_id', 'revision_id.key']]\n",
    "    neg_txt_df = neg_df[['page_id', 'revision_id', 'revision_id.key']]\n",
    "    txt_df = pd.read_csv(txt_fname, index_col=0)\n",
    "\n",
    "    pos_txt_df = pos_txt_df.merge(txt_df, on=['revision_id', 'page_id'])\n",
    "    neg_txt_df = neg_txt_df.merge(txt_df, on=['revision_id', 'page_id'])\n",
    "\n",
    "    pairs_df= pos_txt_df.merge(neg_txt_df, left_on='revision_id.key', right_on='revision_id', suffixes=('_pos', '_neg'))\n",
    "    pairs_df = pairs_df[['page_id_pos', 'revision_id_pos', 'revision_id_neg', 'txt_pos', 'txt_neg']]\n",
    "    pairs_df.rename(columns={'page_id_pos': 'page_id'}, inplace=True)\n",
    "    \n",
    "    return pairs_df\n",
    "\n",
    "def get_diffd_sections(txt1, txt2):\n",
    "    changed_txt1=[]\n",
    "    changed_txt2=[]\n",
    "    for l in difflib.ndiff(txt1.splitlines(), txt2.splitlines()):\n",
    "        if l[0] =='-':\n",
    "            changed_txt1.append(l)\n",
    "        elif l[0]=='+':\n",
    "            changed_txt2.append(l)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    difftxt1=\" \".join([l[2:] for l in changed_txt1])\n",
    "    difftxt2=\" \".join([l[2:] for l in changed_txt2])\n",
    "    \n",
    "    return difftxt1, difftxt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get diff txt for all templates\n",
    "for TEMPLATE_NAME in TEMPLATES_LIST:\n",
    "    print(\"========================================\")\n",
    "    print(TEMPLATE_NAME)\n",
    "    \n",
    "    pairs_df= get_posneg_df(TEMPLATE_NAME)\n",
    "    \n",
    "    # Get difftxt\n",
    "    pairs_df.dropna(subset=['txt_pos', 'txt_neg'], inplace=True)\n",
    "    pairs_df[['difftxt_pos', 'difftxt_neg']]=pairs_df.progress_apply(lambda x: get_diffd_sections(x.txt_pos, x.txt_neg), axis=1, result_type=\"expand\")\n",
    "    diffpairs_df = pairs_df[['page_id', 'revision_id_pos', 'revision_id_neg', 'difftxt_pos', 'difftxt_neg']]\n",
    "\n",
    "    # Save to CSV\n",
    "    diffpairs_df.to_csv(TEMPLATE_DIFFTEXT_DIR+'{}_difftxt.csv.gz'.format(TEMPLATE_NAME), compression='gzip')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>revision_id_pos</th>\n",
       "      <th>revision_id_neg</th>\n",
       "      <th>difftxt_pos</th>\n",
       "      <th>difftxt_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>78379</td>\n",
       "      <td>395882430</td>\n",
       "      <td>396758655</td>\n",
       "      <td>The C2 was a crossbar-interconnected multiproc...</td>\n",
       "      <td>The C2 was a crossbar-interconnected multiproc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>78449</td>\n",
       "      <td>200813468</td>\n",
       "      <td>236697432</td>\n",
       "      <td>thumb|right|350px||   thumb|right|350px| A dev...</td>\n",
       "      <td>thumb|right|350px| thumb|350px| Classification...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>78750</td>\n",
       "      <td>130310859</td>\n",
       "      <td>145429379</td>\n",
       "      <td>\"The Raven\" is a narrative poem by American wr...</td>\n",
       "      <td>thumb|right|Etching of \"The Raven\" by Édouard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>78865</td>\n",
       "      <td>225407201</td>\n",
       "      <td>355702250</td>\n",
       "      <td>Argus McSwine is a fictional character from th...</td>\n",
       "      <td>Argus McSwine is a fictional character from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>79308</td>\n",
       "      <td>642363816</td>\n",
       "      <td>663408380</td>\n",
       "      <td>Skunk species vary in size from about  and in ...</td>\n",
       "      <td>Skunk species vary in size from about  long an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      page_id  revision_id_pos  revision_id_neg  \\\n",
       "1001    78379        395882430        396758655   \n",
       "1002    78449        200813468        236697432   \n",
       "1003    78750        130310859        145429379   \n",
       "1004    78865        225407201        355702250   \n",
       "1005    79308        642363816        663408380   \n",
       "\n",
       "                                            difftxt_pos  \\\n",
       "1001  The C2 was a crossbar-interconnected multiproc...   \n",
       "1002  thumb|right|350px||   thumb|right|350px| A dev...   \n",
       "1003  \"The Raven\" is a narrative poem by American wr...   \n",
       "1004  Argus McSwine is a fictional character from th...   \n",
       "1005  Skunk species vary in size from about  and in ...   \n",
       "\n",
       "                                            difftxt_neg  \n",
       "1001  The C2 was a crossbar-interconnected multiproc...  \n",
       "1002  thumb|right|350px| thumb|350px| Classification...  \n",
       "1003  thumb|right|Etching of \"The Raven\" by Édouard ...  \n",
       "1004  Argus McSwine is a fictional character from th...  \n",
       "1005  Skunk species vary in size from about  long an...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check output\n",
    "test=pd.read_csv(TEMPLATES_DIR+'text/{}_pairs_difftxt.csv.gz'.format('original_research'), index_col=0, nrows=1000)\n",
    "test.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
